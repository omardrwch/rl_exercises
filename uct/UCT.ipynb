{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/omardrwch/rl_exercises.git > /dev/null 2>&1\n",
    "# !cd rl_exercises && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, './rl_exercises/uct/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCT (UCB applied to Trees)\n",
    "\n",
    "In this practical exercice a Monte Carlo Tree Search (MCTS) algorithm called UCT. Given a fixed state $s$, UCT simulates trajectories starting from $s$ and, after $N$ trajectories, recommends an action.\n",
    "\n",
    "UCT is based on 3 functions:\n",
    "\n",
    "* search(state, depth): traverses the tree and returns estimated value of a `state` at given `depth`\n",
    "\n",
    "* select_action(state, depth): chooses an action to be played in `state` at given `depth`\n",
    "\n",
    "* evaluate(state, depth): returns an estimate of the value of `state`  \n",
    "\n",
    "At each iteration of UCT, we start by calling `search` at the root `(state, 0)` and we traverse the tree by choosing actions with the function `select_action`, until a leaf is reached. The cell bellow gives a pseudocode for the function `search`.\n",
    "\n",
    "More details can be found in [this paper](http://ggp.stanford.edu/readings/uct.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "search(state, depth):\n",
    "    if isTerminal(state):\n",
    "        return 0\n",
    "    \n",
    "    if is_leaf(state, depth) or depth >= max_depth:\n",
    "        return evaluate(state, depth)\n",
    "    \n",
    "    action = select_action(state, depth)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    q = reward + gamma*search(next_state, depth + 1)\n",
    "    \n",
    "    update_statistics(state, action, q, depth)\n",
    "    \n",
    "    return q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How can we implement the function `evaluate`?\n",
    "\n",
    "### Question 2: How do we choose an action in the function `select_action`?\n",
    "\n",
    "### Question 3: After $N$ calls to the function `search(s, 0)`, how do we recommend an action?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.envs import SimpleGridWorld\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the environment - A simple GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Set of actions: [0, 1, 2, 3]\n",
      "Number of states:  18\n",
      "Number of actions:  4\n",
      "P has shape:  (18, 4, 18)\n",
      "discount factor:  0.99\n",
      "\n",
      "initial state:  0\n",
      "reward at (s=1, a=3,s'=2):  -1\n",
      "\n",
      "policy that always goes to the right =  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "(s, a, s', r):\n",
      "0 1 1 -1\n",
      "1 1 7 0.0\n",
      "7 1 8 0.0\n",
      "8 1 9 0.0\n",
      "9 1 10 0.0\n",
      "\n",
      "Visualization:\n",
      "o  -  -  -  -  + \n",
      "o  o  o  o  A  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "Envinment set to a given state: \n",
      "o  -  -  -  A  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(gamma=0.99, success_probability=0.8)\n",
    "\n",
    "\n",
    "# Meaning of the actions\n",
    "actions_str = ['left', 'right', 'up', 'down']\n",
    "\n",
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methodsstate\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=1, a=3,s'=2): \", env.reward_fn(1,3,2))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.ones(env.Ns, dtype=np.int32)\n",
    "print(\"policy that always goes to the right = \", policy)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(5):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward)\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n",
    "\n",
    "# Visualizing the environment\n",
    "try:\n",
    "    print(\"Visualization:\")\n",
    "    env.render()\n",
    "except:\n",
    "    pass # render not available\n",
    "\n",
    "# Put envinronment in a given state:\n",
    "target_state = 4\n",
    "env.reset(target_state)\n",
    "print(\"Envinment set to a given state: \")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implement UCT and test it on a GridWorld\n",
    "\n",
    "Try different parameters and check their impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCT:\n",
    "    def __init__(self, env, max_depth = 10, exploration_coeff = 1.0):\n",
    "        self.env = deepcopy(env)\n",
    "        self.max_depth = max_depth\n",
    "        self.exploration_coeff = exploration_coeff\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        Ns = self.env.Ns\n",
    "        Na = self.env.Na\n",
    "        # Number of visits to (s, a, d)\n",
    "        self.N_sad = np.zeros((Ns, Na, self.max_depth))\n",
    "        # Number of visits to (s, d)\n",
    "        self.N_sd = np.zeros((Ns, self.max_depth))\n",
    "        # Sum of rewards obtained at (s, a, d)\n",
    "        self.S_sad = np.zeros((Ns, Na, self.max_depth))\n",
    "        # Is leaf\n",
    "        self.L_sd = np.ones((Ns, self.max_depth))\n",
    "        self.L_sd[:, 0] = 0\n",
    "    \n",
    "    def select_action(self, state, depth):\n",
    "        \"\"\"\n",
    "        TO BE IMPLEMENTED\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def update(self, state, action, q, depth):\n",
    "        \"\"\"\n",
    "        TO BE IMPLEMENTED\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def evaluate(self, state, depth):\n",
    "        \"\"\"\n",
    "        TO BE IMPLEMENTED\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def search(self, state, depth):\n",
    "        \"\"\"\n",
    "        TO BE IMPLEMENTED\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def get_action_recommendation(self, state, depth=0):\n",
    "        \"\"\"\n",
    "        TO BE IMPLEMENTED\n",
    "        \"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uct = UCT(env)\n",
    "n_steps = 5\n",
    "\n",
    "state = env.reset()\n",
    "for ii in range(n_steps):\n",
    "    # run some iterations of UCT\n",
    "    # ...\n",
    "    \n",
    "    action = uct.get_action_recommendation(state)\n",
    "    env.render()\n",
    "    print(\"\")\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
